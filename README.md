# LLM RAG avec Ollama

Syst√®me de Retrieval-Augmented Generation (RAG) utilisant Ollama et Gradio pour interroger des documents PDF avec des mod√®les de langage locaux.

## Description

Ce projet permet d'interroger le contenu d'un document PDF en utilisant :

- **Ollama** pour les mod√®les de langage locaux (deepseek-r1:1.5b, gemma3n:e2b)
- **LangChain** pour le traitement des documents et la cr√©ation d'embeddings
- **ChromaDB** comme base de donn√©es vectorielle
- **Gradio** pour l'interface utilisateur web

## Fonctionnalit√©s

- üìÑ Traitement automatique de documents PDF
- üîç Recherche s√©mantique dans le contenu
- ü§ñ R√©ponses g√©n√©r√©es par des LLM locaux
- üåê Interface web simple et intuitive
- üíæ Base de donn√©es vectorielle persistante

## Installation

### Pr√©requis

1. **Ollama** install√© sur votre syst√®me

   ```bash
   # Installation d'Ollama (macOS)
   curl -fsSL https://ollama.ai/install.sh | sh
   ```

2. **Mod√®les Ollama** requis
   ```bash
   ollama pull deepseek-r1:1.5b
   ollama pull gemma3n:e2b
   ollama pull bge-m3
   ```

### D√©pendances Python

```bash
pip install ollama
pip install langchain chromadb gradio
pip install -U langchain-community
pip install pymupdf
```

## Utilisation

1. **Lancer le notebook**

   ```bash
   jupyter notebook main.ipynb
   ```

2. **Ex√©cuter les cellules** dans l'ordre pour :

   - Installer les d√©pendances
   - Traiter votre document PDF
   - Cr√©er la base de donn√©es vectorielle
   - Lancer l'interface Gradio

3. **Utiliser l'interface web** pour poser des questions sur votre document

## Structure du projet

```
llm_RAG/
‚îú‚îÄ‚îÄ main.ipynb              # Notebook principal
‚îú‚îÄ‚îÄ README.md               # Documentation
‚îú‚îÄ‚îÄ Rapport Alternance S6 - T√©o Viglietti.pdf  # Document exemple
‚îî‚îÄ‚îÄ chroma_db/              # Base de donn√©es vectorielle
    ‚îú‚îÄ‚îÄ chroma.sqlite3
    ‚îî‚îÄ‚îÄ ec7953eb-c7f8-43e1-b5aa-e8dcea7dd548/
```

## Configuration

### Mod√®les utilis√©s

- **Embeddings** : `bge-m3` (via Ollama)
- **Chat** : `deepseek-r1:1.5b` (tests)
- **RAG** : `gemma3n:e2b` (r√©ponses principales)

### Param√®tres de chunking

- **Taille des chunks** : 500 caract√®res
- **Overlap** : 100 caract√®res
- **Nombre de documents r√©cup√©r√©s** : 4

## Fonctionnement

1. **Traitement du PDF** : Le document est divis√© en chunks avec PyMuPDF
2. **Embeddings** : Conversion des chunks en vecteurs avec bge-m3
3. **Stockage** : Sauvegarde dans ChromaDB
4. **Recherche** : R√©cup√©ration des passages pertinents
5. **G√©n√©ration** : Cr√©ation de la r√©ponse avec le contexte

## Interface

L'interface Gradio permet de :

- Saisir une question en fran√ßais
- Recevoir une r√©ponse bas√©e sur le contenu du document
- Historique des conversations

## Exemple d'utilisation

```python
# Question sur le document
question = "Application de suivi des t√¢ches des RDG"

# R√©cup√©ration des documents pertinents
docs = retriever.get_relevant_documents(question, k=4)

# G√©n√©ration de la r√©ponse
response = ollama_llm(question, combined_text)
```

## Auteur

T√©o Viglietti

## Licence

Ce projet est √† des fins √©ducatives et de d√©monstration.
