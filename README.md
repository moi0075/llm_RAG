# LLM RAG avec Ollama

SystÃ¨me RAG (Retrieval-Augmented Generation) pour interroger des PDFs avec Ollama + Gradio.

## ï¿½ Installation Rapide

1. **Ollama + ModÃ¨les**

   ```bash
   # Installer Ollama
   curl -fsSL https://ollama.ai/install.sh | sh

   # TÃ©lÃ©charger les modÃ¨les
   ollama pull gemma3n:e2b
   ollama pull bge-m3
   ```

2. **DÃ©pendances Python**
   ```bash
   pip install ollama langchain chromadb gradio langchain-community pymupdf
   ```

## ğŸ“ Structure

```
llm_RAG/
â”œâ”€â”€ main.ipynb          # Notebook principal
â”œâ”€â”€ pdf/               # Dossier contenant vos PDFs
â””â”€â”€ chroma_db/         # Base vectorielle (auto-gÃ©nÃ©rÃ©e)
```

## ğŸ¯ Utilisation

1. **Placez vos PDFs** dans le dossier `./pdf/`
2. **ExÃ©cutez le notebook** `main.ipynb` cellule par cellule
3. **Utilisez l'interface Gradio** pour poser vos questions

## âš™ï¸ FonctionnalitÃ©s

- ğŸ“„ **Multi-PDFs** : Traite automatiquement tous les PDFs du dossier
- ğŸ” **Recherche vectorielle** : ChromaDB + embeddings bge-m3
- ğŸ¤– **LLM local** : RÃ©ponses en franÃ§ais avec gemma3n:e2b
- ğŸŒ **Interface web** : Gradio simple et intuitive
- ğŸ’¾ **Base persistante** : Pas besoin de retraiter Ã  chaque fois

## ğŸ› ï¸ Configuration

- **Chunks** : 500 caractÃ¨res (overlap 100)
- **Recherche** : 4 documents les plus pertinents
- **ModÃ¨le** : gemma3n:e2b (franÃ§ais)

## ğŸ“ Exemple

```python
# Traitement automatique des PDFs
text_splitter, vectorstore, retriever = process_all_pdfs("./pdf")

# Question
question = "Quelle est la hauteur du platane ?"
docs = retriever.get_relevant_documents(question, k=4)
response = ollama_llm(question, combine_docs(docs))
```

**Auteur** : TÃ©o Viglietti
